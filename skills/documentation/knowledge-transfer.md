---
name: engram-knowledge-transfer
description: "Create onboarding documentation, system overviews, troubleshooting guides, and runbooks. Build queryable knowledge base for team learning and support."
---

# Knowledge Transfer (Engram-Integrated)

## Overview

Create comprehensive knowledge documentation by systematically capturing system overviews, onboarding guides, troubleshooting procedures, and operational runbooks. Store knowledge in Engram as queryable context to enable effective team onboarding, incident response, and continuous learning.

## When to Use

Use this skill when:
- Onboarding new team members to the codebase
- Documenting complex system behavior or architecture
- Creating runbooks for operational procedures (deployment, rollback)
- Building troubleshooting guides for common issues
- Capturing tribal knowledge before team members leave
- After resolving a complex incident (post-mortem documentation)
- Setting up knowledge base for customer support
- Documenting undocumented legacy systems

## The Pattern

### Step 1: Create System Overview

Start with high-level system documentation:

```bash
engram context create \
  --title "System Overview: [System Name]" \
  --content "## Purpose\n\n**What:** [What is this system?]\n\n**Why:** [Why does it exist? What problem does it solve?]\n\n**Who:** [Who uses it? Internal team? External customers?]\n\n## Architecture\n\n**High-Level Diagram:**\n\n\`\`\`\n[ASCII diagram or description]\n\n┌─────────────┐\n│   Clients   │ (Web, Mobile, API)\n└──────┬──────┘\n       │\n       ▼\n┌──────────────────┐\n│   API Gateway    │ (Authentication, Rate Limiting)\n└────┬─────────────┘\n     │\n     ├─────────────┬─────────────┬─────────────┐\n     ▼             ▼             ▼             ▼\n┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐\n│ Service │  │ Service │  │ Service │  │ Service │\n│   A     │  │   B     │  │   C     │  │   D     │\n└────┬────┘  └────┬────┘  └────┬────┘  └────┬────┘\n     │            │            │            │\n     └────────────┴────────────┴────────────┘\n                    │\n                    ▼\n           ┌─────────────────┐\n           │    Database     │\n           │  + Redis Cache  │\n           └─────────────────┘\n\`\`\`\n\n**Components:**\n\n### API Gateway\n- **Technology:** Nginx + custom auth middleware\n- **Responsibilities:** Authentication, rate limiting, routing\n- **Location:** `infrastructure/gateway/`\n- **Docs:** [Link to detailed docs]\n\n### Service A: User Service\n- **Technology:** Rust + Actix-web\n- **Responsibilities:** User authentication, profile management\n- **Location:** `services/user-service/`\n- **Database:** PostgreSQL (users, profiles)\n- **Key Files:**\n  - `services/user-service/src/handlers/auth.rs` - Authentication logic\n  - `services/user-service/src/models/user.rs` - User data model\n- **Docs:** [Link to service README]\n\n### Service B: Document Service\n- **Technology:** Rust + Actix-web\n- **Responsibilities:** Document CRUD, sharing, search\n- **Location:** `services/document-service/`\n- **Database:** PostgreSQL (documents, shares)\n- **Key Files:**\n  - `services/document-service/src/handlers/documents.rs`\n  - `services/document-service/src/search.rs` - Full-text search\n\n### Service C: Notification Service\n- **Technology:** Python + FastAPI\n- **Responsibilities:** Email, push notifications, webhooks\n- **Location:** `services/notification-service/`\n- **Queue:** RabbitMQ (async notification processing)\n\n### Service D: Analytics Service\n- **Technology:** Go\n- **Responsibilities:** Event tracking, metrics aggregation\n- **Location:** `services/analytics-service/`\n- **Database:** ClickHouse (time-series events)\n\n## Data Flow\n\n**Example: Create Document**\n\n1. Client sends POST /api/v1/documents with JWT token\n2. API Gateway validates JWT, forwards to Document Service\n3. Document Service:\n   - Validates request (title required, content length)\n   - Checks user authorization\n   - Creates document in PostgreSQL\n   - Publishes event to RabbitMQ (document.created)\n4. Notification Service:\n   - Consumes document.created event\n   - Sends email to document owner\n5. Analytics Service:\n   - Consumes document.created event\n   - Records event in ClickHouse\n6. Document Service returns 201 Created to client\n\n## Key Concepts\n\n### Authentication\n- JWT tokens with 15-minute expiry\n- Refresh tokens with 7-day expiry\n- Tokens signed with RS256 (asymmetric)\n- Public key distributed to services for verification\n\n### Authorization\n- Role-based access control (RBAC)\n- Roles: user, premium, moderator, admin\n- Permissions checked at service level (defense-in-depth)\n\n### Data Consistency\n- Strong consistency within service (PostgreSQL ACID)\n- Eventual consistency across services (events via RabbitMQ)\n- Idempotent event handlers (handle duplicate events)\n\n### Scalability\n- Services are stateless (horizontal scaling)\n- Database read replicas for read-heavy queries\n- Redis cache for hot data (user sessions, rate limits)\n\n## Technology Stack\n\n**Languages:** Rust, Python, Go\n**Frameworks:** Actix-web, FastAPI\n**Databases:** PostgreSQL, Redis, ClickHouse\n**Message Queue:** RabbitMQ\n**Infrastructure:** Kubernetes on AWS EKS\n**Monitoring:** Prometheus + Grafana\n**Logging:** ELK stack (Elasticsearch, Logstash, Kibana)\n**CI/CD:** GitHub Actions → AWS ECR → Kubernetes\n\n## Development Environment\n\n**Prerequisites:**\n- Rust 1.75+\n- Python 3.11+\n- Go 1.21+\n- Docker + Docker Compose\n- kubectl\n\n**Local Setup:**\n\n\`\`\`bash\n# Clone repo\ngit clone https://github.com/example/platform.git\ncd platform\n\n# Start local services\ndocker-compose up -d\n\n# Run migrations\n./scripts/migrate.sh\n\n# Run tests\n./scripts/test.sh\n\n# Start services\n./scripts/dev.sh\n\`\`\`\n\n**Access:**\n- API Gateway: http://localhost:8000\n- User Service: http://localhost:8001\n- Document Service: http://localhost:8002\n- RabbitMQ UI: http://localhost:15672 (guest/guest)\n- PostgreSQL: localhost:5432 (postgres/postgres)\n\n## Important Links\n\n- **GitHub:** https://github.com/example/platform\n- **Docs:** https://docs.example.com\n- **API Docs:** https://api-docs.example.com\n- **Grafana:** https://grafana.example.com\n- **Kibana:** https://kibana.example.com\n- **Runbooks:** [Link to runbook context]\n- **ADRs:** [Link to ADR list]\n- **Slack:** #platform-eng" \
  --source "knowledge-transfer" \
  --tags "knowledge-base,overview,[system-name]"
```

### Step 2: Create Onboarding Guide

Document step-by-step onboarding for new team members:

```bash
engram context create \
  --title "Onboarding Guide: [Team/Project Name]" \
  --content "## Welcome to [Team Name]!\n\nThis guide will help you get up to speed on our systems, processes, and culture.\n\n## Week 1: Environment Setup and Codebase Familiarization\n\n### Day 1: Access and Tools\n\n**Accounts to Request:**\n- [ ] GitHub org access (ask @tech-lead)\n- [ ] AWS console access (ask @devops-lead)\n- [ ] Slack channels: #platform-eng, #incidents, #deploys\n- [ ] Grafana + Kibana (SSO with company email)\n- [ ] PagerDuty (if on-call rotation)\n\n**Tools to Install:**\n\n\`\`\`bash\n# Development tools\nbrew install rust python go docker kubectl\n\n# Code formatting\ncargo install rustfmt\npip install black\ngo install golang.org/x/tools/cmd/goimports@latest\n\n# Database client\nbrew install postgresql\n\n# API testing\nbrew install httpie\n\`\`\`\n\n**Clone Repositories:**\n\n\`\`\`bash\ngit clone git@github.com:example/platform.git\ngit clone git@github.com:example/infrastructure.git\ngit clone git@github.com:example/docs.git\n\`\`\`\n\n### Day 2: Run Locally\n\n**Follow Local Setup Guide:**\n\n1. Read System Overview (see context: \"System Overview: Platform\")\n2. Start local environment:\n   \`\`\`bash\n   cd platform\n   docker-compose up -d\n   ./scripts/migrate.sh\n   ./scripts/dev.sh\n   \`\`\`\n3. Test API:\n   \`\`\`bash\n   http POST :8000/auth/register email=test@example.com password=test123 name=\"Test User\"\n   http POST :8000/auth/login email=test@example.com password=test123\n   http GET :8000/users/me Authorization:\"Bearer <token>\"\n   \`\`\`\n\n**Troubleshooting:**\n\nIf services fail to start, see \"Troubleshooting Guide: Local Development\"\n\n### Day 3-5: Code Reading\n\n**Read in This Order:**\n\n1. **Architecture:** Read ADRs in `docs/adr/`\n   - Start with ADR-001 (Database choice)\n   - ADR-003 (Microservices architecture)\n   - ADR-005 (Event-driven communication)\n\n2. **User Service:**\n   - `services/user-service/README.md`\n   - `services/user-service/src/main.rs` (entry point)\n   - `services/user-service/src/handlers/auth.rs` (authentication)\n   - Run tests: `cd services/user-service && cargo test`\n\n3. **Document Service:**\n   - `services/document-service/README.md`\n   - `services/document-service/src/handlers/documents.rs` (CRUD)\n   - `services/document-service/src/search.rs` (full-text search)\n\n4. **API Design:**\n   - Read OpenAPI spec: `docs/openapi.yaml`\n   - Try API examples in docs\n\n**Questions to Answer:**\n\n- How are JWT tokens generated and verified?\n- How does document sharing work?\n- What happens when a document is created? (trace event flow)\n- How are permissions enforced?\n\n**Pair Programming:**\n\nSchedule 2-hour pairing sessions with:\n- Backend lead (service architecture)\n- DevOps lead (deployment process)\n\n## Week 2: First Contribution\n\n### Pick Starter Task\n\n**Good First Issues:**\n\nLook for GitHub issues tagged `good-first-issue`:\n- Add validation to API endpoint\n- Fix typo in documentation\n- Add test coverage for existing code\n- Refactor small function\n\n**Workflow:**\n\n1. Create feature branch:\n   \`\`\`bash\n   git checkout -b fix/issue-123-add-validation\n   \`\`\`\n\n2. Make changes, write tests:\n   \`\`\`bash\n   cargo test  # Run tests locally\n   cargo fmt   # Format code\n   cargo clippy  # Lint\n   \`\`\`\n\n3. Commit with conventional commits:\n   \`\`\`bash\n   git commit -m \"fix(user-service): add email validation\n   \n   - Add regex validation for email format\n   - Add test cases for invalid emails\n   - Return 400 error with clear message\n   \n   Fixes #123\"\n   \`\`\`\n\n4. Push and create PR:\n   \`\`\`bash\n   git push origin fix/issue-123-add-validation\n   # Open PR in GitHub, request review from team lead\n   \`\`\`\n\n5. Address review feedback:\n   \`\`\`bash\n   # Make changes\n   git commit -m \"address review feedback\"\n   git push\n   \`\`\`\n\n6. Merge when approved (squash and merge)\n\n### Code Review Process\n\n**As Author:**\n- Write clear PR description (what, why, how)\n- Link to issue or ADR\n- Add screenshots if UI change\n- Ensure CI passes (tests, linting)\n- Respond to feedback promptly\n\n**As Reviewer:**\n- Check for correctness (does it solve the problem?)\n- Check for tests (is new code tested?)\n- Check for clarity (is code readable?)\n- Check for security (any vulnerabilities?)\n- Approve when satisfied, request changes otherwise\n\n## Week 3-4: Deeper Dive\n\n### Operational Tasks\n\n**Learn Deployment Process:**\n\n1. Read runbook: \"Deployment Runbook\"\n2. Shadow a deployment (watch team lead deploy)\n3. Deploy to staging yourself (with guidance)\n4. Deploy to production yourself (with approval)\n\n**Learn Monitoring:**\n\n1. Explore Grafana dashboards\n   - Service health dashboard\n   - API latency dashboard\n   - Error rate dashboard\n2. Explore Kibana logs\n   - Search for your user ID\n   - Find logs for specific request_id\n3. Set up alerts (PagerDuty, Slack)\n\n**Learn Incident Response:**\n\n1. Read runbook: \"Incident Response\"\n2. Shadow an incident (observe on-call engineer)\n3. Do tabletop exercise (simulate incident)\n\n### Contribute to Knowledge Base\n\n**Document What You Learned:**\n\nAs a new team member, you have fresh eyes. Document things that were confusing:\n\n- Update README if setup instructions were unclear\n- Add FAQ section for common questions\n- Write troubleshooting guide for issues you encountered\n- Create diagrams for complex flows\n\n**Example:**\n\n\`\`\`bash\n# After solving a problem, document it\nengram context create \\\n  --title \"Troubleshooting: Docker Compose Fails on Mac\" \\\n  --content \"[Problem, solution, why it works]\" \\\n  --tags \"troubleshooting,docker,mac\"\n\`\`\`\n\n## Month 2+: Become Independent\n\n### Take on Larger Tasks\n\n- Implement new feature (multi-day task)\n- Fix complex bug (requires investigation)\n- Refactor legacy code (improve architecture)\n- Write ADR for design decision\n- Mentor newer team members\n\n### Participate in On-Call Rotation\n\n- Read all runbooks\n- Shadow on-call engineer for 1 week\n- Take on-call shift with backup (someone else on standby)\n- Eventually: solo on-call shift\n\n### Continuous Learning\n\n**Recommended Reading:**\n- \"Designing Data-Intensive Applications\" (Martin Kleppmann)\n- \"Release It!\" (Michael Nygard)\n- \"Site Reliability Engineering\" (Google SRE book)\n\n**Internal Docs to Read:**\n- All ADRs in `docs/adr/`\n- All runbooks (see context list)\n- Post-mortems from past incidents\n\n**Team Rituals:**\n- Daily standup (9:00 AM)\n- Sprint planning (Monday)\n- Retrospective (Friday)\n- Tech talks (bi-weekly)\n- Lunch and learn (monthly)\n\n## Culture and Norms\n\n**Communication:**\n- Slack for quick questions (#platform-eng)\n- GitHub discussions for design proposals\n- Video calls for complex discussions\n- Async by default (respect timezones)\n\n**Code Quality:**\n- All code must be reviewed\n- All code must have tests\n- CI must pass before merge\n- Production requires 2 approvals\n\n**Work-Life Balance:**\n- Normal hours: 9 AM - 5 PM (flexible)\n- On-call: 1 week every 6 weeks\n- Incidents: escalate if needed, don't hero\n- Vacation: take it! Notify team in advance\n\n## Getting Help\n\n**Stuck on Something?**\n\n1. Check docs first (System Overview, ADRs, Runbooks)\n2. Search Slack history (might be answered already)\n3. Ask in #platform-eng (explain what you tried)\n4. Pair with teammate (schedule sync)\n5. Escalate to team lead if urgent\n\n**Don't Hesitate to Ask:**\n\nEveryone was new once. Asking questions helps improve docs for future team members.\n\n## Checklist: Ready to Go Solo\n\nAfter 1-2 months, you should be able to:\n\n- [ ] Set up local development environment from scratch\n- [ ] Understand system architecture (services, data flow)\n- [ ] Make code changes and create PRs\n- [ ] Deploy to staging\n- [ ] Deploy to production (with approval)\n- [ ] Debug issues using logs and metrics\n- [ ] Respond to incidents using runbooks\n- [ ] Write tests and documentation\n- [ ] Participate in design discussions\n- [ ] Mentor new team members\n\n**Congratulations! You're now a full member of the team!**" \
  --source "knowledge-transfer" \
  --tags "knowledge-base,onboarding,[team-name]"
```

### Step 3: Create Troubleshooting Guide

Document common issues and solutions:

```bash
engram context create \
  --title "Troubleshooting Guide: [System/Component Name]" \
  --content "## Troubleshooting Guide\n\nThis guide helps you diagnose and fix common issues.\n\n## Issue: Service Fails to Start Locally\n\n### Symptoms\n\n\`\`\`bash\n./scripts/dev.sh\n# Error: connection refused to PostgreSQL\n\`\`\`\n\n### Diagnosis\n\n**Check if Docker services are running:**\n\n\`\`\`bash\ndocker-compose ps\n\`\`\`\n\nExpected output:\n\`\`\`\nNAME                STATUS              PORTS\npostgres            Up 2 minutes        0.0.0.0:5432->5432/tcp\nredis               Up 2 minutes        0.0.0.0:6379->6379/tcp\nrabbitmq            Up 2 minutes        0.0.0.0:5672->5672/tcp\n\`\`\`\n\n### Solution 1: Docker Not Running\n\n**If no services listed:**\n\n\`\`\`bash\n# Start Docker daemon\nopen -a Docker  # macOS\n# or\nsudo systemctl start docker  # Linux\n\n# Wait for Docker to start, then:\ndocker-compose up -d\n\`\`\`\n\n### Solution 2: Port Already in Use\n\n**If error says \"port 5432 already allocated\":**\n\n\`\`\`bash\n# Find process using port\nlsof -i :5432\n# Kill it or stop your local PostgreSQL:\nbrew services stop postgresql\n\n# Or change port in docker-compose.yml:\nports:\n  - \"5433:5432\"  # Use different host port\n\n# Update DATABASE_URL in .env:\nDATABASE_URL=postgresql://postgres:postgres@localhost:5433/platform\n\`\`\`\n\n### Solution 3: Database Not Initialized\n\n**If tables don't exist:**\n\n\`\`\`bash\n# Run migrations\n./scripts/migrate.sh\n\n# Or manually:\ncd services/user-service\nsqlx migrate run\n\`\`\`\n\n**Why This Works:**\n\nMigrations create database schema (tables, indexes). Service expects schema to exist.\n\n---\n\n## Issue: Request Returns 500 Internal Server Error\n\n### Symptoms\n\n\`\`\`bash\nhttp POST :8000/documents title=\"Test\"\n# HTTP/1.1 500 Internal Server Error\n\`\`\`\n\n### Diagnosis\n\n**Check service logs:**\n\n\`\`\`bash\n# Docker Compose\ndocker-compose logs -f document-service\n\n# Or direct logs\ntail -f logs/document-service.log\n\`\`\`\n\n**Look for error:**\n\n\`\`\`json\n{\n  \"level\": \"error\",\n  \"request_id\": \"req_abc123\",\n  \"error\": \"database error: connection pool exhausted\"\n}\n\`\`\`\n\n### Solution 1: Database Connection Pool Exhausted\n\n**If logs show \"connection pool exhausted\":**\n\n\`\`\`bash\n# Check active connections\npsql -U postgres -d platform -c \"SELECT count(*) FROM pg_stat_activity;\"\n\n# If near max_connections (default 100):\n# 1. Restart service (clears stale connections)\ndocker-compose restart document-service\n\n# 2. Or increase pool size in config:\n# config/document-service.toml\n[database]\nmax_connections = 20  # Increase from 10\n\`\`\`\n\n**Why This Works:**\n\nConnection pool limits concurrent DB connections. If service doesn't release connections (bug) or load is high, pool exhausts.\n\n### Solution 2: Database Migration Missing\n\n**If logs show \"relation does not exist\":**\n\n\`\`\`bash\n# Run latest migrations\n./scripts/migrate.sh\n\n# Check applied migrations\npsql -U postgres -d platform -c \"SELECT version FROM schema_migrations ORDER BY version;\"\n\`\`\`\n\n**Why This Works:**\n\nCode may reference table that doesn't exist yet. Migration creates it.\n\n---\n\n## Issue: JWT Token Rejected (401 Unauthorized)\n\n### Symptoms\n\n\`\`\`bash\nhttp GET :8000/documents Authorization:\"Bearer eyJhbGc...\"\n# HTTP/1.1 401 Unauthorized\n# {\"error\": {\"code\": \"INVALID_TOKEN\", \"message\": \"Token signature invalid\"}}\n\`\`\`\n\n### Diagnosis\n\n**Check token expiry:**\n\n\`\`\`bash\n# Decode JWT (without verifying)\necho \"eyJhbGc...\" | cut -d. -f2 | base64 -d | jq .\n# Look at \"exp\" field (Unix timestamp)\n\n# Compare to current time\ndate +%s\n\`\`\`\n\n### Solution 1: Token Expired\n\n**If exp < current time:**\n\nToken is expired. Get new token:\n\n\`\`\`bash\n# Use refresh token\nhttp POST :8000/auth/refresh refresh_token=\"rt_abc123\"\n\n# Or login again\nhttp POST :8000/auth/login email=user@example.com password=secret\n\`\`\`\n\n### Solution 2: Public Key Mismatch\n\n**If token not expired but still rejected:**\n\nService may be using old public key.\n\n\`\`\`bash\n# Check public key in service config\ncat services/document-service/config/public_key.pem\n\n# Should match auth service public key\ncat services/auth-service/keys/public_key.pem\n\n# If different, copy from auth service:\ncp services/auth-service/keys/public_key.pem services/document-service/config/\n\n# Restart service\ndocker-compose restart document-service\n\`\`\`\n\n**Why This Works:**\n\nJWT is signed with auth service private key. Other services verify with public key. If public key changes (key rotation), services need updated key.\n\n---\n\n## Issue: Full-Text Search Returns No Results\n\n### Symptoms\n\n\`\`\`bash\nhttp GET ':8000/documents?q=project'\n# Returns empty array but documents exist with \"project\" in title\n\`\`\`\n\n### Diagnosis\n\n**Check search_vector column:**\n\n\`\`\`bash\npsql -U postgres -d platform\n\nSELECT id, title, search_vector FROM documents LIMIT 5;\n# If search_vector is NULL, not indexed\n\`\`\`\n\n### Solution: Rebuild Search Index\n\n\`\`\`sql\n-- Update search_vector for all documents\nUPDATE documents\nSET search_vector = to_tsvector('english', title || ' ' || content);\n\n-- Verify\nSELECT id, title FROM documents\nWHERE search_vector @@ to_tsquery('english', 'project');\n\`\`\`\n\n**Why This Works:**\n\nPostgreSQL full-text search uses tsvector column. If NULL, search fails. Trigger should update automatically but may have failed.\n\n**Permanent Fix:**\n\nEnsure trigger exists:\n\n\`\`\`sql\nCREATE TRIGGER update_search_vector\nBEFORE INSERT OR UPDATE ON documents\nFOR EACH ROW\nEXECUTE FUNCTION tsvector_update_trigger(search_vector, 'pg_catalog.english', title, content);\n\`\`\`\n\n---\n\n## Issue: Event Not Processed (Message Stuck in Queue)\n\n### Symptoms\n\nDocument created but no notification email sent.\n\n### Diagnosis\n\n**Check RabbitMQ:**\n\n1. Open http://localhost:15672 (guest/guest)\n2. Go to Queues tab\n3. Check notification_service_queue\n   - If \"Ready\" count > 0, messages waiting\n   - If \"Unacked\" count > 0, messages being processed\n\n**Check consumer logs:**\n\n\`\`\`bash\ndocker-compose logs -f notification-service\n# Look for errors processing events\n\`\`\`\n\n### Solution 1: Consumer Not Running\n\n**If no consumer logs:**\n\n\`\`\`bash\n# Restart service\ndocker-compose restart notification-service\n\n# Check if consuming\ndocker-compose logs -f notification-service | grep \"Connected to RabbitMQ\"\n\`\`\`\n\n### Solution 2: Event Handler Failing\n\n**If logs show repeated errors:**\n\n\`\`\`bash\n# Example error: \"SMTP connection failed\"\n# Fix SMTP config in .env\nSMTP_HOST=smtp.gmail.com\nSMTP_PORT=587\nSMTP_USERNAME=...\nSMTP_PASSWORD=...\n\n# Restart\ndocker-compose restart notification-service\n\n# Retry failed messages (in RabbitMQ UI)\n# Go to queue → Get Message → Requeue\n\`\`\`\n\n**Why This Works:**\n\nEvent handler fails, RabbitMQ requeues message. If persistent failure (bad config), messages accumulate. Fix config, then retry.\n\n---\n\n## General Debugging Tips\n\n### Enable Debug Logging\n\n\`\`\`bash\n# In .env or config file\nRUST_LOG=debug\nLOG_LEVEL=debug\n\n# Restart service\ndocker-compose restart document-service\n\n# Watch logs\ndocker-compose logs -f document-service\n\`\`\`\n\n### Use Request ID for Tracing\n\n**Every API response includes request_id:**\n\n\`\`\`bash\nhttp GET :8000/documents\n# X-Request-ID: req_abc123\n\`\`\`\n\n**Search logs for request_id:**\n\n\`\`\`bash\ngrep req_abc123 logs/*.log\n# Shows all log lines for that request across services\n\`\`\`\n\n### Check Service Health\n\n\`\`\`bash\n# Health check endpoint\nhttp GET :8000/health\n# {\"status\": \"healthy\", \"database\": \"connected\", \"cache\": \"connected\"}\n\n# If unhealthy, check component:\nhttp GET :8000/health/database\n\`\`\`\n\n### Database Query Performance\n\n**If queries slow:**\n\n\`\`\`sql\n-- Find slow queries\nSELECT query, calls, mean_exec_time, max_exec_time\nFROM pg_stat_statements\nORDER BY mean_exec_time DESC\nLIMIT 10;\n\n-- Explain query\nEXPLAIN ANALYZE SELECT * FROM documents WHERE owner_id = '...';\n-- Look for \"Seq Scan\" (bad) vs \"Index Scan\" (good)\n\n-- Add index if needed\nCREATE INDEX idx_documents_owner_id ON documents(owner_id);\n\`\`\`\n\n## Still Stuck?\n\n1. Check if similar issue reported in GitHub issues\n2. Ask in Slack #platform-eng with:\n   - What you're trying to do\n   - What's happening (error message, logs)\n   - What you've tried\n   - Request ID if applicable\n3. Pair with teammate to debug together\n4. Create incident if production issue" \
  --source "knowledge-transfer" \
  --tags "knowledge-base,troubleshooting,[system-name]"
```

### Step 4: Create Operational Runbook

Document procedures for deployments, rollbacks, incidents:

```bash
engram context create \
  --title "Runbook: Deployment Procedure" \
  --content "## Deployment Runbook\n\nThis runbook guides you through deploying code to staging and production.\n\n## Prerequisites\n\n- [ ] Code reviewed and approved (2+ approvals for production)\n- [ ] CI tests passing (all checks green)\n- [ ] Staging deployed and tested\n- [ ] No active incidents (check #incidents channel)\n- [ ] Change window (production deploys: Tuesday-Thursday, 10 AM - 4 PM)\n\n## Staging Deployment\n\n**Frequency:** Continuous (after every merge to main)\n\n**Automated via CI/CD:**\n\n\`\`\`yaml\n# .github/workflows/deploy-staging.yml\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Build and push Docker image\n        run: |\n          docker build -t platform:${{ github.sha }} .\n          docker push $ECR_REGISTRY/platform:${{ github.sha }}\n      - name: Deploy to staging\n        run: |\n          kubectl set image deployment/document-service \\\n            document-service=$ECR_REGISTRY/platform:${{ github.sha }} \\\n            --namespace=staging\n      - name: Wait for rollout\n        run: |\n          kubectl rollout status deployment/document-service -n staging --timeout=5m\n      - name: Run smoke tests\n        run: |\n          ./scripts/smoke-tests.sh https://api-staging.example.com\n\`\`\`\n\n**Manual Verification:**\n\n\`\`\`bash\n# After automated deploy, verify:\n\n# 1. Check deployment status\nkubectl get pods -n staging\n# All pods should be Running\n\n# 2. Check logs for errors\nkubectl logs -f deployment/document-service -n staging | grep -i error\n# Should be no errors\n\n# 3. Test API\nhttp POST https://api-staging.example.com/auth/login \\\n  email=test@example.com password=test123\n\nhttp GET https://api-staging.example.com/documents \\\n  Authorization:\"Bearer <token>\"\n\n# 4. Check metrics\n# Grafana: Staging Dashboard\n# - Request rate normal?\n# - Error rate < 1%?\n# - P99 latency < 500ms?\n\`\`\`\n\n## Production Deployment\n\n**Frequency:** Multiple times per day (during change window)\n\n**Change Window:** Tuesday-Thursday, 10 AM - 4 PM (avoid Mondays and Fridays)\n\n### Step 1: Pre-Deployment Checklist\n\n- [ ] Staging deployment successful and tested\n- [ ] No active incidents (check Grafana, PagerDuty)\n- [ ] No planned maintenance (check #ops channel)\n- [ ] Database migrations tested on staging\n- [ ] Rollback plan documented (see \"Rollback Procedure\" below)\n- [ ] Announce in #deploys: \"Deploying document-service vX.Y.Z to prod\"\n\n### Step 2: Database Migration (if needed)\n\n**IMPORTANT: Run migrations BEFORE deploying code.**\n\n\`\`\`bash\n# Connect to production database (read-write replica)\nexport DATABASE_URL=\"postgresql://deployer@prod-db.example.com/platform\"\n\n# Dry-run migration (no changes)\nsqlx migrate run --dry-run\n\n# Review SQL to be executed\ncat migrations/010_add_document_tags.sql\n\n# Run migration\nsqlx migrate run\n\n# Verify migration applied\npsql $DATABASE_URL -c \"SELECT version FROM schema_migrations ORDER BY version DESC LIMIT 5;\"\n\n# Test backward compatibility\n# Old code should still work with new schema\ncurl https://api.example.com/documents\n\`\`\`\n\n**Migration Guidelines:**\n- Add columns as nullable (required columns break old code)\n- Don't remove columns until old code fully deployed\n- Test migration on staging with production-size data\n- Have rollback migration ready\n\n### Step 3: Deploy Code\n\n**Option A: Automated (GitHub Actions)**\n\n\`\`\`bash\n# Tag release\ngit tag v1.2.3\ngit push origin v1.2.3\n\n# GitHub Actions automatically deploys tagged releases to production\n# Monitor: https://github.com/example/platform/actions\n\`\`\`\n\n**Option B: Manual (Emergency)**\n\n\`\`\`bash\n# Build image\ndocker build -t platform:v1.2.3 .\ndocker push $ECR_REGISTRY/platform:v1.2.3\n\n# Deploy to production\nkubectl set image deployment/document-service \\\n  document-service=$ECR_REGISTRY/platform:v1.2.3 \\\n  --namespace=production\n\n# Watch rollout\nkubectl rollout status deployment/document-service -n production --timeout=10m\n\`\`\`\n\n### Step 4: Monitor Deployment\n\n**Watch Grafana Dashboard:**\n\nProduction Dashboard: https://grafana.example.com/d/prod\n\n**Metrics to Watch:**\n\n| Metric | Expected | Action if Anomaly |\n|--------|----------|------------------|\n| Request rate | Steady (no drop) | Rollback if drop >20% |\n| Error rate | < 1% | Rollback if >5% |\n| P99 latency | < 200ms | Investigate if >500ms |\n| Pod restarts | 0 | Check logs |\n| CPU usage | < 70% | Scale up if >90% |\n| Memory usage | < 80% | Check for memory leak |\n\n**Watch Logs:**\n\n\`\`\`bash\n# Tail logs from new pods\nkubectl logs -f deployment/document-service -n production | grep -i \"error\\|panic\\|fatal\"\n\n# Or use Kibana\n# https://kibana.example.com\n# Query: namespace:production AND level:error\n\`\`\`\n\n**Smoke Tests:**\n\n\`\`\`bash\n# Run automated smoke tests\n./scripts/smoke-tests.sh https://api.example.com\n\n# Or manual tests\nhttp POST https://api.example.com/auth/login email=test@example.com password=test123\nhttp GET https://api.example.com/documents Authorization:\"Bearer <token>\"\nhttp POST https://api.example.com/documents title=\"Test\" Authorization:\"Bearer <token>\"\n\`\`\`\n\n### Step 5: Post-Deployment Verification\n\n**5 minutes after deploy:**\n\n- [ ] Error rate normal (< 1%)\n- [ ] Latency normal (P99 < 200ms)\n- [ ] No errors in logs\n- [ ] All pods running\n- [ ] Smoke tests pass\n\n**15 minutes after deploy:**\n\n- [ ] Metrics stable\n- [ ] No alerts fired\n- [ ] No customer complaints (#support channel)\n\n**Announce in #deploys:**\n\n\"Document-service v1.2.3 deployed to prod successfully. Metrics normal.\"\n\n## Rollback Procedure\n\n**When to Rollback:**\n- Error rate > 5%\n- P99 latency > 500ms\n- Critical functionality broken\n- Database corruption detected\n\n**How to Rollback:**\n\n\`\`\`bash\n# Find previous working version\nkubectl rollout history deployment/document-service -n production\n# REVISION  IMAGE\n# 45        platform:v1.2.2\n# 46        platform:v1.2.3 (current, broken)\n\n# Rollback to previous revision\nkubectl rollout undo deployment/document-service -n production\n\n# Or rollback to specific revision\nkubectl rollout undo deployment/document-service -n production --to-revision=45\n\n# Watch rollout\nkubectl rollout status deployment/document-service -n production\n\n# Verify metrics recover\n# Grafana: Production Dashboard\n\n# Announce in #incidents:\n# \"Rolled back document-service from v1.2.3 to v1.2.2 due to high error rate. Investigating.\"\n\`\`\`\n\n**Database Migration Rollback:**\n\nIf migration caused issue:\n\n\`\`\`bash\n# Run down migration\npsql $DATABASE_URL < migrations/010_add_document_tags.down.sql\n\n# Or use migration tool\nsqlx migrate revert\n\n# Verify rollback\npsql $DATABASE_URL -c \"\\d documents\"\n\`\`\`\n\n**Post-Rollback:**\n\n1. Create incident ticket\n2. Document what went wrong\n3. Fix issue in main branch\n4. Test on staging\n5. Re-deploy when fix confirmed\n\n## Emergency Procedures\n\n### Emergency Rollback (P0 Incident)\n\n**If production is down:**\n\n\`\`\`bash\n# Immediate rollback (don't wait for analysis)\nkubectl rollout undo deployment/document-service -n production\n\n# Notify team\n# Slack: @here Document-service rolled back due to P0 incident\n\n# Page on-call if needed\n# PagerDuty: Trigger incident\n\n# Start incident response (see \"Incident Response Runbook\")\n\`\`\`\n\n### Emergency Hotfix\n\n**If critical bug in production:**\n\n1. Create hotfix branch from production tag:\n   \`\`\`bash\n   git checkout -b hotfix/critical-bug v1.2.3\n   \`\`\`\n\n2. Fix bug, commit:\n   \`\`\`bash\n   git commit -m \"hotfix: fix critical bug causing outage\"\n   \`\`\`\n\n3. Tag and push:\n   \`\`\`bash\n   git tag v1.2.4\n   git push origin hotfix/critical-bug v1.2.4\n   \`\`\`\n\n4. Deploy immediately (skip normal change window):\n   \`\`\`bash\n   # Automated deploy triggers\n   # Or manual deploy\n   \`\`\`\n\n5. Merge back to main:\n   \`\`\`bash\n   git checkout main\n   git merge hotfix/critical-bug\n   git push\n   \`\`\`\n\n## Troubleshooting Deployments\n\n### Pod Stuck in CrashLoopBackOff\n\n\`\`\`bash\n# Check logs\nkubectl logs deployment/document-service -n production --previous\n# --previous shows logs from crashed container\n\n# Describe pod\nkubectl describe pod -l app=document-service -n production\n# Look for \"Events\" section\n\n# Common causes:\n# - Missing environment variable\n# - Database connection failed\n# - Config file missing\n\`\`\`\n\n### Rollout Stuck at 50%\n\n\`\`\`bash\n# Check rollout status\nkubectl rollout status deployment/document-service -n production\n\n# If stuck, check pod health:\nkubectl get pods -n production\n# Some pods may be \"Running\" but failing health checks\n\n# Check health endpoint\nkubectl exec -it <pod-name> -n production -- curl localhost:8080/health\n\n# If health check fails, rollback\nkubectl rollout undo deployment/document-service -n production\n\`\`\`\n\n## Contacts\n\n- **On-Call Engineer:** See PagerDuty schedule\n- **DevOps Lead:** @devops-lead (Slack)\n- **Engineering Manager:** @eng-manager (Slack)\n- **Escalation:** Page CTO if P0 incident > 1 hour" \
  --source "knowledge-transfer" \
  --tags "knowledge-base,runbook,deployment,[system-name]"
```

### Step 5: Link All Knowledge Transfer Entities

```bash
# Link knowledge base to system
engram relationship create \
  --source-id [SYSTEM_ID] --source-type context \
  --target-id [OVERVIEW_ID] --target-type context \
  --relationship-type references --agent [AGENT]

engram relationship create \
  --source-id [SYSTEM_ID] --source-type context \
  --target-id [ONBOARDING_ID] --target-type context \
  --relationship-type references --agent [AGENT]

engram relationship create \
  --source-id [SYSTEM_ID] --source-type context \
  --target-id [TROUBLESHOOTING_ID] --target-type context \
  --relationship-type references --agent [AGENT]

engram relationship create \
  --source-id [SYSTEM_ID] --source-type context \
  --target-id [RUNBOOK_ID] --target-type context \
  --relationship-type references --agent [AGENT]
```

## Querying Knowledge Base

After creating knowledge base, agents and team members can query:

```bash
# Get system overview
engram context list | grep "System Overview"

# Get onboarding guides
engram context list | grep "Onboarding"

# Get troubleshooting guides
engram context list --tags troubleshooting

# Get runbooks
engram context list --tags runbook

# Search for specific topic
engram context list | grep -i "deployment"
engram context list | grep -i "docker"

# Get all knowledge for a system
engram relationship connected --entity-id [SYSTEM_ID] --relationship-type references
```

## Knowledge Base Maintenance

**Keep Documentation Up-to-Date:**

1. **During Code Changes:**
   - Update docs when changing functionality
   - Add troubleshooting steps for bugs you fix
   - Document workarounds for known issues

2. **During Onboarding:**
   - New team members should update docs that were unclear
   - Add FAQ entries for questions they asked

3. **After Incidents:**
   - Document incident in post-mortem
   - Add runbook entries for how to detect/fix
   - Update troubleshooting guide with new issues

4. **Regular Reviews:**
   - Quarterly: Review and update all runbooks
   - After major changes: Update system overview
   - When tools change: Update onboarding guide

## Related Skills

This skill integrates with:
- `engram-adr` - ADRs provide decision context for knowledge base
- `engram-api-docs` - API documentation is part of knowledge base
- `engram-system-design` - System architecture docs aid understanding
- `engram-use-engram-memory` - Store all knowledge in Engram for querying
- `engram-audit-trail` - Track who contributed knowledge when
